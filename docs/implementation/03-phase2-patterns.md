# Phase 2: æ¨¡å¼è¯†åˆ«å¼•æ“

**ç›®æ ‡**: å®ç° AI é©±åŠ¨çš„æ¨¡å¼è¯†åˆ«å¼•æ“ï¼Œè‡ªåŠ¨å‘ç°ç”¨æˆ·çš„é‡å¤æ“ä½œæ¨¡å¼å¹¶å»ºè®®è‡ªåŠ¨åŒ–

**é¢„è®¡æ—¶é—´**: 20-25 å¤©

---

## ğŸ“‹ æ¦‚è¿°

æœ¬é˜¶æ®µæ˜¯ FlowMind çš„**æ ¸å¿ƒåˆ›æ–°åŠŸèƒ½**ï¼Œå°†å®ç°ï¼š

1. **äº‹ä»¶åºåˆ—å­˜å‚¨** - å°†ç›‘æ§äº‹ä»¶æŒä¹…åŒ–åˆ° SQLite æ•°æ®åº“
2. **ä¼šè¯åˆ’åˆ†** - å°†äº‹ä»¶æŒ‰æ—¶é—´çª—å£åˆ†ç»„ï¼Œè¯†åˆ«å·¥ä½œä¼šè¯
3. **æ¨¡å¼æŒ–æ˜** - ä½¿ç”¨ PrefixSpan ç®—æ³•è¯†åˆ«é¢‘ç¹åºåˆ—
4. **AI è¿‡æ»¤** - ä½¿ç”¨ Claude API åˆ¤æ–­æ¨¡å¼æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–
5. **æ¨¡å¼å»ºè®®** - å‘ç”¨æˆ·å±•ç¤ºå‘ç°çš„æ¨¡å¼å¹¶ç”Ÿæˆè‡ªåŠ¨åŒ–å»ºè®®

### ç³»ç»Ÿæ¶æ„

```
Monitor Engine (å·²å®ç°)
    â†“ å‘å¸ƒäº‹ä»¶
Event Bus (pkg/events)
    â†“ è®¢é˜…äº‹ä»¶
Analyzer Engine (æ–°å¢)
  â”œâ”€ EventRepository    # äº‹ä»¶æŒä¹…åŒ– (SQLite)
  â”œâ”€ SessionDivider     # ä¼šè¯åˆ’åˆ† (æ—¶é—´çª—å£)
  â”œâ”€ PatternMiner       # PrefixSpan ç®—æ³•
  â”œâ”€ AIPatternFilter    # Claude API é›†æˆ
  â””â”€ PatternRecommender # å»ºè®®ç”Ÿæˆ
    â†“ è¾“å‡ºæ¨¡å¼å»ºè®®
Frontend UI
```

---

## ğŸš€ å®æ–½æ­¥éª¤

### Step 1: æ·»åŠ ä¾èµ– (1 å¤©)

**ä»»åŠ¡æ¸…å•**:
- [ ] æ›´æ–° `go.mod` æ·»åŠ ä¾èµ–:
  ```bash
  go get github.com/mattn/go-sqlite3
  go get go.etcd.io/bbolt
  go get github.com/philippgille/chromem-go
  go mod tidy
  ```
- [ ] éªŒè¯ä¾èµ–å®‰è£…: `go build ./...`

**éªŒè¯æ ‡å‡†**:
- æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ
- é¡¹ç›®å¯ä»¥æ­£å¸¸ç¼–è¯‘

---

### Step 2: å®ç°å­˜å‚¨å±‚ (3-4 å¤©)

#### Day 1: SQLite åŸºç¡€è®¾æ–½

**æ–‡ä»¶ç»“æ„**:
```
internal/storage/
â”œâ”€â”€ sqlite.go                  # SQLite è¿æ¥ç®¡ç†
â”œâ”€â”€ migrations.go              # è¿ç§»æ‰§è¡Œå™¨
â””â”€â”€ migrations/
    â””â”€â”€ 001_init.sql           # events è¡¨
```

**å…³é”®ä»£ç **:

`sqlite.go`:
```go
// NewSQLiteDB åˆ›å»ºæ•°æ®åº“è¿æ¥
//
// Parameters:
//   - dbPath: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
//
// Returns: *sql.DB - æ•°æ®åº“è¿æ¥å®ä¾‹, error - é”™è¯¯ä¿¡æ¯
func NewSQLiteDB(dbPath string) (*sql.DB, error) {
    db, err := sql.Open("sqlite3", dbPath)
    if err != nil {
        return nil, fmt.Errorf("æ‰“å¼€æ•°æ®åº“å¤±è´¥: %w", err)
    }

    // é…ç½® WAL æ¨¡å¼ (æå‡å¹¶å‘æ€§èƒ½)
    db.Exec("PRAGMA journal_mode=WAL")
    db.Exec("PRAGMA synchronous=NORMAL")
    db.Exec("PRAGMA cache_size=10000")
    db.SetMaxOpenConns(25) // è¿æ¥æ± é…ç½®

    return db, nil
}
```

`migrations/001_init.sql`:
```sql
-- äº‹ä»¶è¡¨
CREATE TABLE IF NOT EXISTS events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid TEXT UNIQUE NOT NULL,
    type TEXT NOT NULL,
    timestamp DATETIME NOT NULL,
    data JSON,
    application TEXT,
    bundle_id TEXT,
    window_title TEXT,
    file_path TEXT,
    selection TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_type ON events(type);
CREATE INDEX idx_events_application ON events(application);
CREATE INDEX idx_events_uuid ON events(uuid);
```

**éªŒè¯æ ‡å‡†**:
- [ ] æ•°æ®åº“è¿æ¥æˆåŠŸ
- [ ] è¿ç§»æ‰§è¡ŒæˆåŠŸ
- [ ] events è¡¨åˆ›å»ºæˆåŠŸ
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡

---

#### Day 2: EventRepository

**æ–‡ä»¶ç»“æ„**:
```
internal/storage/
â””â”€â”€ event_repository.go        # äº‹ä»¶ä»“å‚¨æ¥å£
```

**æ ¸å¿ƒæ¥å£**:
```go
// EventRepository äº‹ä»¶å­˜å‚¨æ¥å£
type EventRepository interface {
    // Save ä¿å­˜å•ä¸ªäº‹ä»¶
    Save(event *events.Event) error

    // SaveBatch æ‰¹é‡ä¿å­˜äº‹ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    SaveBatch(events []events.Event) error

    // FindByTimeRange æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢
    FindByTimeRange(start, end time.Time) ([]events.Event, error)

    // FindRecent æŸ¥è¯¢æœ€è¿‘çš„äº‹ä»¶
    FindRecent(limit int) ([]events.Event, error)

    // FindByType æŒ‰ç±»å‹æŸ¥è¯¢
    FindByType(eventType events.EventType, limit int) ([]events.Event, error)

    // DeleteOlderThan åˆ é™¤æ—§æ•°æ®
    DeleteOlderThan(cutoff time.Time) (int64, error)

    // GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
    GetStats() (*EventStats, error)
}
```

**æ‰¹é‡å†™å…¥ä¼˜åŒ–**:
```go
// SaveBatch æ‰¹é‡ä¿å­˜äº‹ä»¶
//
// ä½¿ç”¨äº‹åŠ¡å’Œé¢„å¤„ç†è¯­å¥ä¼˜åŒ–æ‰¹é‡å†™å…¥æ€§èƒ½
//
// Parameters:
//   - events: äº‹ä»¶æ•°ç»„
//
// Returns: error - é”™è¯¯ä¿¡æ¯
func (r *SQLiteEventRepository) SaveBatch(events []events.Event) error {
    tx, _ := r.db.Begin()
    defer tx.Rollback()

    stmt, _ := tx.Prepare(`
        INSERT INTO events (uuid, type, timestamp, data, application, ...)
        VALUES (?, ?, ?, ?, ?, ...)
    `)
    defer stmt.Close()

    for _, event := range events {
        stmt.Exec(event.ID, event.Type, event.Timestamp, event.Data, ...)
    }

    return tx.Commit()
}
```

**éªŒè¯æ ‡å‡†**:
- [ ] æ‰€æœ‰æ¥å£å®ç°å®Œæˆ
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 90%
- [ ] æ‰¹é‡å†™å…¥æ€§èƒ½ > 1000 events/sec

---

#### Day 3: è¿ç§»è„šæœ¬

**æ–‡ä»¶ç»“æ„**:
```
internal/storage/migrations/
â”œâ”€â”€ 002_add_sessions.sql       # sessions è¡¨
â”œâ”€â”€ 003_add_patterns.sql       # patterns è¡¨
â”œâ”€â”€ 004_add_automations.sql    # automations è¡¨
â”œâ”€â”€ 005_add_ai_cache.sql       # AI ç¼“å­˜è¡¨
â””â”€â”€ 006_schema_migrations.sql  # è¿ç§»è®°å½•è¡¨
```

**å…³é”® SQL**:

`002_add_sessions.sql`:
```sql
CREATE TABLE IF NOT EXISTS sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid TEXT UNIQUE NOT NULL,
    application TEXT NOT NULL,
    bundle_id TEXT,
    start_time DATETIME NOT NULL,
    end_time DATETIME,
    event_count INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_sessions_time ON sessions(start_time);
CREATE INDEX idx_sessions_application ON sessions(application);
```

`003_add_patterns.sql`:
```sql
CREATE TABLE IF NOT EXISTS patterns (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid TEXT UNIQUE NOT NULL,
    name TEXT,
    sequence_hash TEXT UNIQUE NOT NULL,
    sequence JSON NOT NULL,
    support_count INTEGER DEFAULT 1,
    confidence REAL DEFAULT 0.0,
    first_seen DATETIME NOT NULL,
    last_seen DATETIME NOT NULL,
    is_automated BOOLEAN DEFAULT FALSE,
    automation_id INTEGER,
    ai_analysis TEXT,
    estimated_time_saving INTEGER,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_patterns_automated ON patterns(is_automated);
CREATE INDEX idx_patterns_support ON patterns(support_count);
CREATE INDEX idx_patterns_hash ON patterns(sequence_hash);
```

---

#### Day 4: æ€§èƒ½ä¼˜åŒ–

**æ–‡ä»¶ç»“æ„**:
```
internal/storage/
â””â”€â”€ batch_writer.go            # æ‰¹é‡å†™å…¥å™¨
```

**æ‰¹é‡å†™å…¥å™¨**:
```go
// BatchWriter æ‰¹é‡å†™å…¥å™¨
//
// ä¼˜åŒ–æ•°æ®åº“å†™å…¥æ€§èƒ½ï¼Œé€šè¿‡æ‰¹é‡å†™å…¥å’Œå®šæ—¶åˆ·æ–°å‡å°‘æ•°æ®åº“å‹åŠ›
type BatchWriter struct {
    repo         EventRepository
    buffer       []events.Event
    bufferSize   int           // 100æ¡ä¸€æ‰¹
    flushTimer   *time.Timer   // æˆ–5ç§’åˆ·æ–°
    mu           sync.Mutex
}

// Add æ·»åŠ äº‹ä»¶åˆ°ç¼“å†²åŒº
//
// å½“ç¼“å†²åŒºè¾¾åˆ°æ‰¹é‡å¤§å°æ—¶è‡ªåŠ¨åˆ·æ–°
func (bw *BatchWriter) Add(event events.Event) {
    bw.mu.Lock()
    defer bw.mu.Unlock()

    bw.buffer = append(bw.buffer, event)

    // è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–å®šæ—¶åˆ·æ–°
    if len(bw.buffer) >= bw.bufferSize {
        bw.flush()
    }
}

// flush åˆ·æ–°ç¼“å†²åŒºåˆ°æ•°æ®åº“
func (bw *BatchWriter) flush() {
    if len(bw.buffer) == 0 {
        return
    }

    // å¼‚æ­¥æ‰¹é‡å†™å…¥
    events := make([]events.Event, len(bw.buffer))
    copy(events, bw.buffer)
    bw.buffer = bw.buffer[:0]

    go func() {
        if err := bw.repo.SaveBatch(events); err != nil {
            logger.Error("æ‰¹é‡å†™å…¥å¤±è´¥", zap.Error(err))
        }
    }()
}
```

**éªŒè¯æ ‡å‡†**:
- [ ] æ‰¹é‡å†™å…¥æ€§èƒ½è¾¾æ ‡
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡ (1000 events/sec)

---

### Step 3: é›†æˆå­˜å‚¨å±‚åˆ° Monitor Engine (1 å¤©)

**ä¿®æ”¹æ–‡ä»¶**: `internal/monitor/engine.go`

**å…³é”®ä»£ç **:
```go
// Engine ç›‘æ§å¼•æ“
type Engine struct {
    // ç°æœ‰å­—æ®µ
    keyboard   Monitor
    clipboard  Monitor
    eventBus   *events.EventBus
    isRunning  bool
    mu         sync.RWMutex

    // æ–°å¢å­—æ®µ
    eventRepo   storage.EventRepository
    batchWriter *storage.BatchWriter
}

// NewEngine åˆ›å»ºç›‘æ§å¼•æ“
func NewEngine(eventBus *events.EventBus, eventRepo storage.EventRepository) Monitor {
    return &Engine{
        eventBus:     eventBus,
        eventRepo:    eventRepo,
        batchWriter:  storage.NewBatchWriter(eventRepo, 100, 5*time.Second),
    }
}

// Start å¯åŠ¨ç›‘æ§å¼•æ“
func (e *Engine) Start() error {
    e.mu.Lock()
    defer e.mu.Unlock()

    // ... ç°æœ‰å¯åŠ¨é€»è¾‘

    // è®¢é˜…æ‰€æœ‰äº‹ä»¶å¹¶æŒä¹…åŒ–
    e.eventBus.Subscribe("*", func(event events.Event) error {
        e.batchWriter.Add(event)
        return nil
    })

    e.isRunning = true
    logger.Info("ç›‘æ§å¼•æ“å¯åŠ¨æˆåŠŸ", zap.String("component", "engine"))

    return nil
}
```

**éªŒè¯æ ‡å‡†**:
- [ ] ç›‘æ§äº‹ä»¶è‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
- [ ] å¯ä»¥æŸ¥è¯¢ä¿å­˜çš„äº‹ä»¶
- [ ] æ€§èƒ½æ— æ˜æ˜¾å½±å“

---

### Step 4: å®ç° SessionDivider (2 å¤©)

**æ–‡ä»¶ç»“æ„**:
```
internal/analyzer/
â”œâ”€â”€ types.go                   # å…±äº«ç±»å‹å®šä¹‰
â””â”€â”€ session.go                 # ä¼šè¯åˆ’åˆ†é€»è¾‘
```

**æ ¸å¿ƒç±»å‹** (`types.go`):
```go
// Session ä¼šè¯å®šä¹‰
type Session struct {
    ID          string
    StartTime   time.Time
    EndTime     *time.Time
    Application string
    BundleID    string
    EventCount  int
    Events      []events.Event
}

// SessionDividerConfig ä¼šè¯åˆ’åˆ†é…ç½®
type SessionDividerConfig struct {
    Timeout         time.Duration // ä¼šè¯è¶…æ—¶ (é»˜è®¤10åˆ†é’Ÿ)
    MinEvents       int           // æœ€å°äº‹ä»¶æ•° (é»˜è®¤5)
    AppSwitchBreaks bool          // åº”ç”¨åˆ‡æ¢æ˜¯å¦æ‰“æ–­ä¼šè¯
}
```

**ä¼šè¯åˆ’åˆ†å™¨** (`session.go`):
```go
// SessionDivider ä¼šè¯åˆ’åˆ†å™¨æ¥å£
type SessionDivider interface {
    // Divide åˆ’åˆ†äº‹ä»¶ä¸ºä¼šè¯
    Divide(events []events.Event) ([]*Session, error)

    // GetCurrentSession è·å–å½“å‰æ´»è·ƒä¼šè¯
    GetCurrentSession() (*Session, error)

    // EndSession ç»“æŸå½“å‰ä¼šè¯
    EndSession() error
}

// TimeBasedDivider åŸºäºæ—¶é—´çš„ä¼šè¯åˆ’åˆ†
type TimeBasedDivider struct {
    config SessionDividerConfig
}

// Divide åˆ’åˆ†äº‹ä»¶ä¸ºä¼šè¯
//
// ä½¿ç”¨è¶…æ—¶æ£€æµ‹å’Œåº”ç”¨åˆ‡æ¢æ£€æµ‹æ¥åˆ’åˆ†ä¼šè¯
// 10åˆ†é’Ÿæ— æ“ä½œæˆ–åº”ç”¨åˆ‡æ¢éƒ½ä¼šç»“æŸå½“å‰ä¼šè¯
func (td *TimeBasedDivider) Divide(events []events.Event) ([]*Session, error) {
    if len(events) == 0 {
        return nil, nil
    }

    var sessions []*Session
    current := &Session{
        ID:          uuid.New().String(),
        StartTime:   events[0].Timestamp,
        EndTime:     &events[0].Timestamp,
        Application: events[0].Context.Application,
        BundleID:    events[0].Context.BundleID,
    }

    for _, event := range events {
        // æ£€æŸ¥è¶…æ—¶ (10åˆ†é’Ÿ)
        if event.Timestamp.Sub(*current.EndTime) > td.config.Timeout {
            sessions = append(sessions, current)
            current = td.newSession(event)
            continue
        }

        // æ£€æŸ¥åº”ç”¨åˆ‡æ¢
        if td.config.AppSwitchBreaks &&
           event.Context.Application != current.Application {
            sessions = append(sessions, current)
            current = td.newSession(event)
            continue
        }

        // ç»§ç»­å½“å‰ä¼šè¯
        current.Events = append(current.Events, event)
        current.EventCount++
        current.EndTime = &event.Timestamp
    }

    // æ·»åŠ æœ€åä¸€ä¸ªä¼šè¯
    sessions = append(sessions, current)

    return sessions, nil
}
```

**éªŒè¯æ ‡å‡†**:
- [ ] æ­£ç¡®åˆ’åˆ†ä¼šè¯
- [ ] è¶…æ—¶æ£€æµ‹å‡†ç¡®
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡

---

### Step 5: å®ç° PatternMiner (3-4 å¤©)

**æ–‡ä»¶ç»“æ„**:
```
internal/analyzer/
â”œâ”€â”€ normalizer.go              # äº‹ä»¶æ ‡å‡†åŒ–
â”œâ”€â”€ prefixspan.go              # PrefixSpan ç®—æ³•
â””â”€â”€ pattern_miner.go           # æ¨¡å¼æŒ–æ˜å™¨
```

#### Day 1: äº‹ä»¶æ ‡å‡†åŒ–

**normalizer.go**:
```go
// EventNormalizer äº‹ä»¶æ ‡å‡†åŒ–å™¨
//
// å°†åŸå§‹äº‹ä»¶è½¬æ¢ä¸ºæŠ½è±¡çš„ EventStepï¼Œä¾¿äºæ¨¡å¼æŒ–æ˜
type EventNormalizer struct{}

// EventStep äº‹ä»¶æ­¥éª¤ï¼ˆæŠ½è±¡åŒ–ï¼‰
type EventStep struct {
    Type        string
    Application string
    Data        map[string]interface{}
    Wildcard    bool // æ˜¯å¦ä¸ºé€šé…ç¬¦ï¼ˆåŒ¹é…ä»»æ„ï¼‰
}

// Normalize æ ‡å‡†åŒ–äº‹ä»¶
//
// Parameters:
//   - event: åŸå§‹äº‹ä»¶
//
// Returns: EventStep - æ ‡å‡†åŒ–åçš„æ­¥éª¤
func (en *EventNormalizer) Normalize(event events.Event) EventStep {
    step := EventStep{
        Type:        string(event.Type),
        Application: event.Context.Application,
    }

    // æ ¹æ®äº‹ä»¶ç±»å‹æå–å…³é”®ç‰¹å¾
    switch event.Type {
    case events.EventTypeKeyboard:
        // æå–æŒ‰é”®ç±»å‹ï¼ˆå­—æ¯ã€æ•°å­—ã€ç¬¦å·ã€åŠŸèƒ½é”®ï¼‰
        if keyCode, ok := event.Data["keycode"].(float64); ok {
            step.Data = map[string]interface{}{
                "key_type": en.classifyKey(int(keyCode)),
            }
        }

    case events.EventTypeClipboard:
        step.Data = map[string]interface{}{
            "has_content": true,
        }

    case events.EventTypeAppSwitch:
        // åº”ç”¨åˆ‡æ¢æœ¬èº«å°±æ˜¯æœ‰æ„ä¹‰çš„äº‹ä»¶
    }

    return step
}

// classifyKey åˆ†ç±»æŒ‰é”®
//
// Parameters:
//   - keyCode: æŒ‰é”®ä»£ç 
//
// Returns: string - æŒ‰é”®ç±»å‹ (letter/number/other)
func (en *EventNormalizer) classifyKey(keyCode int) string {
    switch {
    case keyCode >= 0 && keyCode <= 26:
        return "letter"  // A-Z
    case keyCode >= 30 && keyCode <= 39:
        return "number"  // 0-9
    default:
        return "other"
    }
}
```

---

#### Day 2: PrefixSpan ç®—æ³•

**prefixspan.go**:
```go
// PrefixSpan PrefixSpanç®—æ³•å®ç°
type PrefixSpan struct {
    config PatternMinerConfig
}

// Mine ä»ä¼šè¯ä¸­æŒ–æ˜æ¨¡å¼
//
// Parameters:
//   - sessions: ä¼šè¯æ•°ç»„
//
// Returns: []*Pattern - å‘ç°çš„æ¨¡å¼æ•°ç»„, error - é”™è¯¯ä¿¡æ¯
func (ps *PrefixSpan) Mine(sessions []*Session) ([]*Pattern, error) {
    // 1. æ„å»ºåºåˆ—æ•°æ®åº“
    sequences := ps.buildSequences(sessions)

    // 2. é€’å½’æŒ–æ˜é¢‘ç¹æ¨¡å¼
    patterns := make([]*Pattern, 0)
    ps.mineRecursive(sequences, []EventStep{}, &patterns)

    // 3. è®¡ç®—ç½®ä¿¡åº¦
    for _, pattern := range patterns {
        pattern.Confidence = ps.calculateConfidence(pattern, sequences)
    }

    return patterns, nil
}

// mineRecursive é€’å½’æŒ–æ˜é¢‘ç¹æ¨¡å¼
//
// Parameters:
//   - sequences: åºåˆ—æ•°æ®åº“
//   - prefix: å½“å‰å‰ç¼€
//   - patterns: æ¨¡å¼é›†åˆï¼ˆè¾“å‡ºå‚æ•°ï¼‰
func (ps *PrefixSpan) mineRecursive(
    sequences []EventSequence,
    prefix []EventStep,
    patterns *[]Pattern,
) {
    // è®¡ç®—å‰ç¼€æ”¯æŒåº¦
    support := ps.calculateSupport(sequences, prefix)
    if support < ps.config.MinSupport && len(prefix) > 0 {
        return // å‰ªæï¼šæ”¯æŒåº¦ä¸è¶³ï¼Œæå‰ç»ˆæ­¢
    }

    // ä¿å­˜æœ‰æ•ˆæ¨¡å¼
    if len(prefix) >= ps.config.MinPatternLen {
        *patterns = append(*patterns, &Pattern{
            ID:           generatePatternID(prefix),
            Sequence:     prefix,
            SupportCount: support,
        })
    }

    // ç”ŸæˆæŠ•å½±æ•°æ®åº“
    projectedDB := ps.buildProjectedDB(sequences, prefix)

    // æ‰¾åˆ°é¢‘ç¹é¡¹
    frequentItems := ps.findFrequentItems(projectedDB)

    // é€’å½’æŒ–æ˜
    for _, item := range frequentItems {
        newPrefix := append([]EventStep{}, prefix...)
        newPrefix = append(newPrefix, item)

        if len(newPrefix) <= ps.config.MaxPatternLen {
            ps.mineRecursive(projectedDB, newPrefix, patterns)
        }
    }
}

// buildProjectedDB æ„å»ºæŠ•å½±æ•°æ®åº“
//
// Parameters:
//   - sequences: åŸå§‹åºåˆ—æ•°æ®åº“
//   - prefix: å½“å‰å‰ç¼€
//
// Returns: []EventSequence - æŠ•å½±æ•°æ®åº“
func (ps *PrefixSpan) buildProjectedDB(
    sequences []EventSequence,
    prefix []EventStep,
) []EventSequence {
    projected := make([]EventSequence, 0)

    for _, seq := range sequences {
        // æ‰¾åˆ°å‰ç¼€åŒ¹é…ä½ç½®
        index := ps.findPrefixIndex(seq, prefix)

        if index != -1 && index < len(seq.Events)-1 {
            // æŠ•å½±ï¼šä»åŒ¹é…ä½ç½®ä¹‹åçš„äº‹ä»¶
            projected = append(projected, EventSequence{
                Events:    seq.Events[index+1:],
                StartTime: seq.Events[index+1].Timestamp,
                EndTime:   seq.EndTime,
                SessionID: seq.SessionID,
            })
        }
    }

    return projected
}

// calculateSupport è®¡ç®—æ”¯æŒåº¦
//
// Parameters:
//   - sequences: åºåˆ—æ•°æ®åº“
//   - prefix: å‰ç¼€æ¨¡å¼
//
// Returns: int - æ”¯æŒåº¦ï¼ˆåŒ…å«è¯¥å‰ç¼€çš„åºåˆ—æ•°ï¼‰
func (ps *PrefixSpan) calculateSupport(
    sequences []EventSequence,
    prefix []EventStep,
) int {
    if len(prefix) == 0 {
        return len(sequences)
    }

    count := 0
    for _, seq := range sequences {
        if ps.containsPrefix(seq, prefix) {
            count++
        }
    }

    return count
}

// containsPrefix æ£€æŸ¥åºåˆ—æ˜¯å¦åŒ…å«å‰ç¼€
//
// Parameters:
//   - seq: äº‹ä»¶åºåˆ—
//   - prefix: å‰ç¼€æ¨¡å¼
//
// Returns: bool - æ˜¯å¦åŒ…å«
func (ps *PrefixSpan) containsPrefix(
    seq EventSequence,
    prefix []EventStep,
) bool {
    if len(prefix) == 0 {
        return true
    }

    if len(seq.Events) < len(prefix) {
        return false
    }

    // æ»‘åŠ¨çª—å£åŒ¹é…
    j := 0
    for _, event := range seq.Events {
        if j >= len(prefix) {
            break
        }

        if ps.matchStep(event, prefix[j]) {
            j++
        }
    }

    return j == len(prefix)
}

// matchStep åŒ¹é…äº‹ä»¶æ­¥éª¤
//
// Parameters:
//   - event: äº‹ä»¶
//   - step: æ­¥éª¤
//
// Returns: bool - æ˜¯å¦åŒ¹é…
func (ps *PrefixSpan) matchStep(event EventStep, step EventStep) bool {
    // é€šé…ç¬¦åŒ¹é…ä»»æ„
    if step.Wildcard {
        return true
    }

    // ç±»å‹åŒ¹é…
    if event.Type != step.Type {
        return false
    }

    // åº”ç”¨åŒ¹é…ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if step.Application != "" && event.Application != step.Application {
        return false
    }

    return true
}
```

**éªŒè¯æ ‡å‡†**:
- [ ] ç®—æ³•æ­£ç¡®æ€§éªŒè¯
- [ ] æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

---

### Step 6: å®ç° AI Service (2-3 å¤©)

**æ–‡ä»¶ç»“æ„**:
```
internal/ai/
â”œâ”€â”€ claude_client.go           # Claude API å®¢æˆ·ç«¯
â”œâ”€â”€ pattern_filter.go          # æ¨¡å¼è¿‡æ»¤
â””â”€â”€ prompts.go                 # æç¤ºè¯æ¨¡æ¿
```

#### Day 1: Claude å®¢æˆ·ç«¯

**claude_client.go**:
```go
// ClaudeClient Claude API å®¢æˆ·ç«¯
type ClaudeClient struct {
    apiKey     string
    baseURL    string
    httpClient *http.Client
    maxRetries int
}

// NewClaudeClient åˆ›å»º Claude å®¢æˆ·ç«¯
//
// Parameters:
//   - apiKey: Claude API å¯†é’¥
//
// Returns: *ClaudeClient - å®¢æˆ·ç«¯å®ä¾‹
func NewClaudeClient(apiKey string) *ClaudeClient {
    return &ClaudeClient{
        apiKey:     apiKey,
        baseURL:    "https://api.anthropic.com/v1/messages",
        httpClient: &http.Client{Timeout: 60 * time.Second},
        maxRetries: 3,
    }
}

// Complete åŒæ­¥è°ƒç”¨ Claude API
//
// Parameters:
//   - ctx: ä¸Šä¸‹æ–‡
//   - prompt: æç¤ºè¯
//
// Returns: string - AI å“åº”, error - é”™è¯¯ä¿¡æ¯
func (c *ClaudeClient) Complete(ctx context.Context, prompt string) (string, error) {
    request := ClaudeRequest{
        Model:     "claude-3-5-sonnet-20241022",
        MaxTokens: 4096,
        Messages: []ClaudeMessage{
            {Role: "user", Content: prompt},
        },
    }

    body, _ := json.Marshal(request)
    req, _ := http.NewRequestWithContext(
        ctx, "POST", c.baseURL, bytes.NewReader(body),
    )

    req.Header.Set("Content-Type", "application/json")
    req.Header.Set("x-api-key", c.apiKey)
    req.Header.Set("anthropic-version", "2023-06-01")

    // é‡è¯•é€»è¾‘
    var lastErr error
    for i := 0; i < c.maxRetries; i++ {
        resp, err := c.httpClient.Do(req)
        if err == nil && resp.StatusCode == http.StatusOK {
            defer resp.Body.Close()

            var response ClaudeResponse
            if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
                return "", err
            }

            return c.extractContent(response), nil
        }

        lastErr = err
        time.Sleep(time.Second * time.Duration(i+1)) // æŒ‡æ•°é€€é¿
    }

    return "", fmt.Errorf("Claude API è°ƒç”¨å¤±è´¥: %w", lastErr)
}
```

#### Day 2: æç¤ºè¯æ¨¡æ¿

**prompts.go**:
```go
// BuildPatternAnalysisPrompt æ„å»ºæ¨¡å¼åˆ†ææç¤ºè¯
//
// Parameters:
//   - pattern: å¾…åˆ†æçš„æ¨¡å¼
//
// Returns: string - æç¤ºè¯
func BuildPatternAnalysisPrompt(pattern *Pattern) string {
    var stepsDesc []string
    for i, step := range pattern.Sequence {
        stepsDesc = append(stepsDesc, fmt.Sprintf("%d. %s", i+1, describeStep(step)))
    }

    return fmt.Sprintf(`ä½ æ˜¯ä¸€ä¸ªå·¥ä½œæµè‡ªåŠ¨åŒ–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é‡å¤æ“ä½œæ¨¡å¼ã€‚

## æ¨¡å¼ä¿¡æ¯
- **å‡ºç°æ¬¡æ•°**: %d
- **é¦–æ¬¡å‘ç°**: %s
- **æœ€è¿‘å‘ç°**: %s
- **æ¨¡å¼é•¿åº¦**: %d æ­¥

## æ“ä½œæ­¥éª¤
%s

## åˆ†æä»»åŠ¡
è¯·åˆ¤æ–­è¿™ä¸ªæ¨¡å¼æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–ï¼Œå¹¶å›ç­”ï¼š
1. æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–ï¼Ÿï¼ˆè€ƒè™‘é‡å¤é¢‘ç‡å’Œå¤æ‚åº¦ï¼‰
2. å¦‚æœå€¼å¾—ï¼Œä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
3. é¢„è®¡æ¯æ¬¡å¯ä»¥èŠ‚çœå¤šå°‘æ—¶é—´ï¼Ÿ
4. å®ç°å¤æ‚åº¦å¦‚ä½•ï¼Ÿ

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å¤ï¼š
{
  "should_automate": trueæˆ–false,
  "reason": "ç®€çŸ­çš„åŸå› è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰",
  "estimated_time_saving": ç§’æ•°ï¼ˆæ•´æ•°ï¼‰,
  "complexity": "low"æˆ–"medium"æˆ–"high",
  "suggested_name": "æ¨èçš„è‡ªåŠ¨åŒ–åç§°",
  "suggested_steps": [
    {
      "action": "æ“ä½œç±»å‹",
      "params": {"key": "value"}
    }
  ]
}

æ³¨æ„ï¼š
- å¦‚æœæ¨¡å¼è¿‡äºç®€å•ï¼ˆå¦‚å•æ¬¡ç‚¹å‡»ï¼‰ï¼Œshould_automate åº”ä¸º false
- å¦‚æœæ¨¡å¼åŒ…å«ç”¨æˆ·ç‰¹å®šå†…å®¹ï¼ˆå¦‚å…·ä½“æ–‡æœ¬ï¼‰ï¼Œåº”ä½¿ç”¨é€šé…ç¬¦
- estimated_time_saving åº”åŸºäºå®é™…æ“ä½œæ—¶é—´ä¼°ç®—`,
        pattern.SupportCount,
        pattern.FirstSeen.Format("2006-01-02 15:04"),
        pattern.LastSeen.Format("2006-01-02 15:04"),
        len(pattern.Sequence),
        strings.Join(stepsDesc, "\n"),
    )
}

// describeStep æè¿°æ­¥éª¤
func describeStep(step EventStep) string {
    switch step.Type {
    case "keyboard":
        return "é”®ç›˜è¾“å…¥"
    case "clipboard":
        return "å¤åˆ¶/ç²˜è´´"
    case "app_switch":
        return fmt.Sprintf("åˆ‡æ¢åˆ°åº”ç”¨: %s", step.Application)
    default:
        return step.Type
    }
}
```

#### Day 3: æ¨¡å¼è¿‡æ»¤

**pattern_filter.go**:
```go
// AIPatternFilter AIæ¨¡å¼è¿‡æ»¤å™¨
type AIPatternFilter interface {
    // ShouldAutomate åˆ¤æ–­æ¨¡å¼æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–
    ShouldAutomate(pattern *Pattern) (bool, *AIAnalysis, error)

    // AnalyzePattern æ·±åº¦åˆ†ææ¨¡å¼
    AnalyzePattern(pattern *Pattern) (*AIAnalysis, error)
}

// AIAnalysis AIåˆ†æç»“æœ
type AIAnalysis struct {
    ShouldAutomate      bool   `json:"should_automate"`
    Reason              string `json:"reason"`
    Complexity          string `json:"complexity"`          // low/medium/high
    EstimatedTimeSaving int    `json:"estimated_time_saving"` // ç§’
    SuggestedName       string `json:"suggested_name"`
    SuggestedSteps      []Step `json:"suggested_steps"`
}

// Step è‡ªåŠ¨åŒ–æ­¥éª¤
type Step struct {
    Action string                 `json:"action"`
    Params map[string]interface{} `json:"params"`
}

// PatternFilter æ¨¡å¼è¿‡æ»¤å™¨å®ç°
type PatternFilter struct {
    client    *ClaudeClient
    cache     *bbolt.DB
    rateLimiter *RateLimiter
}

// NewPatternFilter åˆ›å»ºæ¨¡å¼è¿‡æ»¤å™¨
func NewPatternFilter(client *ClaudeClient, cacheDB *bbolt.DB) *PatternFilter {
    return &PatternFilter{
        client:      client,
        cache:       cacheDB,
        rateLimiter: NewRateLimiter(3), // æœ€å¤§3ä¸ªå¹¶å‘
    }
}

// ShouldAutomate åˆ¤æ–­æ¨¡å¼æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–
//
// Parameters:
//   - pattern: å¾…åˆ¤æ–­çš„æ¨¡å¼
//
// Returns: bool - æ˜¯å¦å€¼å¾—è‡ªåŠ¨åŒ–, *AIAnalysis - AIåˆ†æç»“æœ, error - é”™è¯¯ä¿¡æ¯
func (pf *PatternFilter) ShouldAutomate(pattern *Pattern) (bool, *AIAnalysis, error) {
    // 1. æ£€æŸ¥ç¼“å­˜
    cacheKey := generatePatternHash(pattern)
    if cached, found := pf.getFromCache(cacheKey); found {
        var analysis AIAnalysis
        json.Unmarshal(cached, &analysis)
        return analysis.ShouldAutomate, &analysis, nil
    }

    // 2. é™æµæ§åˆ¶
    pf.rateLimiter.Acquire()
    defer pf.rateLimiter.Release()

    // 3. è°ƒç”¨ Claude API
    prompt := BuildPatternAnalysisPrompt(pattern)
    response, err := pf.client.Complete(context.Background(), prompt)
    if err != nil {
        return false, nil, fmt.Errorf("Claude API è°ƒç”¨å¤±è´¥: %w", err)
    }

    // 4. è§£æå“åº”
    var analysis AIAnalysis
    if err := json.Unmarshal([]byte(response), &analysis); err != nil {
        return false, nil, fmt.Errorf("è§£æ AI å“åº”å¤±è´¥: %w", err)
    }

    // 5. ä¿å­˜åˆ°ç¼“å­˜
    pf.saveToCache(cacheKey, &analysis, 24*time.Hour)

    return analysis.ShouldAutomate, &analysis, nil
}
```

---

### Step 7: å®ç° Analyzer Engine ä¸»å¼•æ“ (2 å¤©)

**æ–‡ä»¶ç»“æ„**:
```
internal/analyzer/
â”œâ”€â”€ engine.go                  # ä¸»å¼•æ“
â””â”€â”€ config.go                  # é…ç½®ç®¡ç†
```

**engine.go**:
```go
// Engine åˆ†æå¼•æ“
type Engine struct {
    // ç»„ä»¶
    eventRepo    EventRepository
    sessionDiv   SessionDivider
    patternMiner PatternMiner
    aiFilter     AIPatternFilter
    recommender  PatternRecommender

    // é…ç½®
    config *Config

    // çŠ¶æ€
    eventBuffer   []events.Event
    knownPatterns map[string]*Pattern

    // é€šé“
    eventChan     <-chan events.Event
    patternChan   chan *Pattern
    recommendChan chan *Recommendation

    // æ§åˆ¶
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
}

// Config å¼•æ“é…ç½®
type Config struct {
    // EventRepository
    DBPath string

    // SessionDivider
    SessionTimeout time.Duration

    // PatternMiner
    MinSupport     int
    MinConfidence  float64
    MiningInterval time.Duration // æ¨¡å¼æŒ–æ˜é—´éš”

    // AI
    ClaudeAPIKey string

    // BatchWriter
    BatchSize     int
    FlushInterval time.Duration
}

// NewEngine åˆ›å»ºåˆ†æå¼•æ“
//
// Parameters:
//   - config: å¼•æ“é…ç½®
//   - eventBus: äº‹ä»¶æ€»çº¿
//
// Returns: *Engine - å¼•æ“å®ä¾‹, error - é”™è¯¯ä¿¡æ¯
func NewEngine(config *Config, eventBus *events.EventBus) (*Engine, error) {
    // åˆå§‹åŒ–å­˜å‚¨
    eventRepo, err := storage.NewSQLiteEventRepository(config.DBPath)
    if err != nil {
        return nil, err
    }

    // åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    aiClient := ai.NewClaudeClient(config.ClaudeAPIKey)

    // åˆ›å»ºå¼•æ“
    ctx, cancel := context.WithCancel(context.Background())

    return &Engine{
        eventRepo:     eventRepo,
        sessionDiv:    NewSessionDivider(config.SessionTimeout),
        patternMiner:  NewPatternMiner(config.MinSupport, config.MinConfidence),
        aiFilter:      ai.NewPatternFilter(aiClient),
        recommender:   NewRecommender(),
        config:        config,
        knownPatterns: make(map[string]*Pattern),
        ctx:           ctx,
        cancel:        cancel,
    }, nil
}

// Start å¯åŠ¨åˆ†æå¼•æ“
//
// Parameters:
//   - eventBus: äº‹ä»¶æ€»çº¿
//
// Returns: error - é”™è¯¯ä¿¡æ¯
func (e *Engine) Start(eventBus *events.EventBus) error {
    logger.Info("å¯åŠ¨åˆ†æå¼•æ“")

    // åˆ›å»ºäº‹ä»¶é€šé“
    e.eventChan = make(chan events.Event, 1000)

    // è®¢é˜…æ‰€æœ‰ç›‘æ§äº‹ä»¶
    eventBus.Subscribe("*", func(event events.Event) error {
        e.eventChan <- event
        return nil
    })

    // å¯åŠ¨æ‰¹é‡å†™å…¥å™¨
    batchWriter := storage.NewBatchWriter(e.eventRepo, e.config.BatchSize, e.config.FlushInterval)

    // å¯åŠ¨äº‹ä»¶å¤„ç†å¾ªç¯
    e.wg.Add(1)
    go e.processEvents(batchWriter)

    // å¯åŠ¨æ¨¡å¼æŒ–æ˜å¾ªç¯
    e.wg.Add(1)
    go e.miningLoop()

    // å¯åŠ¨AIè¿‡æ»¤å¾ªç¯
    e.wg.Add(1)
    go e.aiFilterLoop()

    return nil
}

// processEvents äº‹ä»¶å¤„ç†å¾ªç¯
func (e *Engine) processEvents(batchWriter *storage.BatchWriter) {
    defer e.wg.Done()

    for {
        select {
        case event := <-e.eventChan:
            // æ·»åŠ åˆ°æ‰¹é‡å†™å…¥
            batchWriter.Add(event)

            // æ·»åŠ åˆ°å†…å­˜ç¼“å†²
            e.eventBuffer = append(e.eventBuffer, event)

            // é™åˆ¶ç¼“å†²åŒºå¤§å°
            if len(e.eventBuffer) > 10000 {
                e.eventBuffer = e.eventBuffer[len(e.eventBuffer)-10000:]
            }

        case <-e.ctx.Done():
            // åˆ·æ–°å‰©ä½™äº‹ä»¶
            batchWriter.Flush()
            return
        }
    }
}

// miningLoop æ¨¡å¼æŒ–æ˜å¾ªç¯
func (e *Engine) miningLoop() {
    defer e.wg.Done()

    ticker := time.NewTicker(e.config.MiningInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            e.minePatterns()

        case <-e.ctx.Done():
            return
        }
    }
}

// minePatterns æŒ–æ˜æ¨¡å¼
func (e *Engine) minePatterns() {
    logger.Info("å¼€å§‹æŒ–æ˜æ¨¡å¼")

    // 1. ä»æ•°æ®åº“æŸ¥è¯¢æœ€è¿‘çš„äº‹ä»¶
    cutoff := time.Now().Add(-24 * time.Hour)
    events, err := e.eventRepo.FindByTimeRange(cutoff, time.Now())
    if err != nil {
        logger.Error("æŸ¥è¯¢äº‹ä»¶å¤±è´¥", zap.Error(err))
        return
    }

    // 2. åˆ’åˆ†ä¼šè¯
    sessions := e.sessionDiv.Divide(events)
    logger.Info("åˆ’åˆ†ä¼šè¯", zap.Int("count", len(sessions)))

    // 3. æŒ–æ˜æ¨¡å¼
    patterns, err := e.patternMiner.Mine(sessions)
    if err != nil {
        logger.Error("æŒ–æ˜æ¨¡å¼å¤±è´¥", zap.Error(err))
        return
    }

    logger.Info("å‘ç°æ¨¡å¼", zap.Int("count", len(patterns)))

    // 4. è¿‡æ»¤å·²çŸ¥æ¨¡å¼
    newPatterns := e.filterKnownPatterns(patterns)

    // 5. å‘é€åˆ°AIè¿‡æ»¤
    for _, pattern := range newPatterns {
        e.patternChan <- pattern
    }
}

// Stop åœæ­¢åˆ†æå¼•æ“
//
// Returns: error - é”™è¯¯ä¿¡æ¯
func (e *Engine) Stop() error {
    logger.Info("åœæ­¢åˆ†æå¼•æ“")

    e.cancel()
    e.wg.Wait()

    return nil
}
```

---

### Step 8-11: å‰ç«¯é›†æˆã€æµ‹è¯•ã€éƒ¨ç½²

è¯¦è§å®æ–½è®¡åˆ’æ–‡æ¡£ã€‚

---

## âœ… éªŒè¯æ ‡å‡†

### åŠŸèƒ½éªŒè¯
- [ ] èƒ½è®°å½•ç”¨æˆ·æ‰€æœ‰å…³é”®æ“ä½œ
- [ ] èƒ½æ£€æµ‹åˆ°ç”¨æˆ·é‡å¤æ“ä½œï¼ˆå¦‚å¯åŠ¨å¼€å‘ç¯å¢ƒï¼‰
- [ ] AI æ­£ç¡®åˆ¤æ–­å“ªäº›æ¨¡å¼å€¼å¾—è‡ªåŠ¨åŒ–
- [ ] UI æ˜¾ç¤ºè‡ªåŠ¨åŒ–å»ºè®®åˆ—è¡¨
- [ ] ç”¨æˆ·å¯ä»¥æ¥å—/æ‹’ç»å»ºè®®

### æ€§èƒ½éªŒè¯
- [ ] äº‹ä»¶æŒä¹…åŒ–: >1000 events/sec
- [ ] æ¨¡å¼æŒ–æ˜: <5s å¤„ç† 1000 äº‹ä»¶
- [ ] å†…å­˜å ç”¨: <100MB
- [ ] CPU ä½¿ç”¨: <10% (ç©ºé—²æ—¶)

### è´¨é‡éªŒè¯
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥80%
- [ ] æ ¸å¿ƒæ¨¡å—è¦†ç›–ç‡ â‰¥90%
- [ ] æ‰€æœ‰ä¸­æ–‡æ³¨é‡Šå®Œæ•´
- [ ] æ–‡æ¡£å®Œæ•´

---

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

1. **å‡†ç¡®æ€§**: èƒ½å‘ç° 90% çš„é‡å¤æ“ä½œæ¨¡å¼
2. **æ€§èƒ½**: åˆ†æ 1 å¤©æ•°æ® <5 ç§’
3. **å®ç”¨æ€§**: AI å»ºè®®çš„è‡ªåŠ¨åŒ–æ¥å—ç‡ >70%
4. **ç¨³å®šæ€§**: è¿ç»­è¿è¡Œ 7 å¤©æ— å´©æºƒ

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [ç³»ç»Ÿæ¶æ„](../architecture/01-system-architecture.md)
- [åˆ†æå¼•æ“](../architecture/03-analyzer-engine.md)
- [AI æœåŠ¡](../architecture/04-ai-service.md)
- [æ•°æ®åº“è®¾è®¡](../design/01-database-design.md)
- [å¼€å‘ç¯å¢ƒæ­å»º](./01-development-setup.md)
- [Phase 1: åŸºç¡€ç›‘æ§](./02-phase1-monitoring.md)

---

**æœ€åæ›´æ–°**: 2026-01-30
